import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf,SparkContext}

/**
  * Created by sruthigelivi on 10/2/16.
  */

object InvertedIndex{
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("Invert Me!!").setMaster("local[2]")
    val sc = new SparkContext(conf)
    val textfile = sc.textFile("input.csv")
    val rows = textfile.count()
    val data = textfile.map(line => line.split(",").map(elem => elem.trim()))
    val cols = data.collect()(0).length

    val header= sc.broadcast(data.first())



    val cells = data.zipWithIndex().flatMap {
      case (row, 1) => IndexedSeq.empty // skipping header row
      case (row, index) => row.zip(header.value).map {
        case (value, column) => value ->(column, index)
      }
    }


    val indexVerbose: RDD[(String, Vector[(String, Long)])] =
      cells.aggregateByKey(zeroValue = Vector.empty[(String, Long)])(
        seqOp = (keys, key) => keys :+ key,
        combOp = (keysA, keysB) => keysA ++ keysB)

  }
}
